{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "makiqfaHCf4J"
   },
   "source": [
    "#Â Practical 2 \n",
    "\n",
    "# Part I. Random Forests, ROC analysis and Grid Search\n",
    "\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "Ensemble learning is the technique of building multiple models, and then combining them in a manner that is likely to produce better results than individual models.  These models don't have to be classifiers, and can be trained to deal with most tasks. A Random Forest uses one type of ensemble method called bagging to learn multiple decision trees. This forest of Decision Trees is then used to predict the output value by avaraging or voting. As to esnure diversity among various trees, each Decision Tree is constructed using a boostrap sample of the training data, and only a random subset of variables is used at each node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_ZtbGpKCf4N"
   },
   "source": [
    "RandomForestClassifier docs: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0G6Ymq6m6h5"
   },
   "source": [
    "Next we will work with the Wisconsin breast cancer diagnosis dataset that consists of some input variables about the morphological features of the breast mass and a target variable of 2 classes for the type of the tumor (benign-0 or malignant-1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "13UgCpXvoPGh"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQSIgoeq9VJF",
    "outputId": "fa572365-b621-4f02-ec0a-b0a33c8e39ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "brc = load_breast_cancer()\n",
    "print(brc.DESCR)\n",
    "X = brc.data\n",
    "y = brc.target\n",
    "\n",
    "# Variable names\n",
    "vnames = brc.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "keD8fwJ3oYow",
    "outputId": "cfb7e27f-1e34-46c2-9faf-6839e92e4aa9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2            3           4   \\\n",
       "count  569.000000  569.000000  569.000000   569.000000  569.000000   \n",
       "mean    14.127292   19.289649   91.969033   654.889104    0.096360   \n",
       "std      3.524049    4.301036   24.298981   351.914129    0.014064   \n",
       "min      6.981000    9.710000   43.790000   143.500000    0.052630   \n",
       "25%     11.700000   16.170000   75.170000   420.300000    0.086370   \n",
       "50%     13.370000   18.840000   86.240000   551.100000    0.095870   \n",
       "75%     15.780000   21.800000  104.100000   782.700000    0.105300   \n",
       "max     28.110000   39.280000  188.500000  2501.000000    0.163400   \n",
       "\n",
       "               5           6           7           8           9   ...  \\\n",
       "count  569.000000  569.000000  569.000000  569.000000  569.000000  ...   \n",
       "mean     0.104341    0.088799    0.048919    0.181162    0.062798  ...   \n",
       "std      0.052813    0.079720    0.038803    0.027414    0.007060  ...   \n",
       "min      0.019380    0.000000    0.000000    0.106000    0.049960  ...   \n",
       "25%      0.064920    0.029560    0.020310    0.161900    0.057700  ...   \n",
       "50%      0.092630    0.061540    0.033500    0.179200    0.061540  ...   \n",
       "75%      0.130400    0.130700    0.074000    0.195700    0.066120  ...   \n",
       "max      0.345400    0.426800    0.201200    0.304000    0.097440  ...   \n",
       "\n",
       "               20          21          22           23          24  \\\n",
       "count  569.000000  569.000000  569.000000   569.000000  569.000000   \n",
       "mean    16.269190   25.677223  107.261213   880.583128    0.132369   \n",
       "std      4.833242    6.146258   33.602542   569.356993    0.022832   \n",
       "min      7.930000   12.020000   50.410000   185.200000    0.071170   \n",
       "25%     13.010000   21.080000   84.110000   515.300000    0.116600   \n",
       "50%     14.970000   25.410000   97.660000   686.500000    0.131300   \n",
       "75%     18.790000   29.720000  125.400000  1084.000000    0.146000   \n",
       "max     36.040000   49.540000  251.200000  4254.000000    0.222600   \n",
       "\n",
       "               25          26          27          28          29  \n",
       "count  569.000000  569.000000  569.000000  569.000000  569.000000  \n",
       "mean     0.254265    0.272188    0.114606    0.290076    0.083946  \n",
       "std      0.157336    0.208624    0.065732    0.061867    0.018061  \n",
       "min      0.027290    0.000000    0.000000    0.156500    0.055040  \n",
       "25%      0.147200    0.114500    0.064930    0.250400    0.071460  \n",
       "50%      0.211900    0.226700    0.099930    0.282200    0.080040  \n",
       "75%      0.339100    0.382900    0.161400    0.317900    0.092080  \n",
       "max      1.058000    1.252000    0.291000    0.663800    0.207500  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a quick look at the data, e.g. the shape of input and output, and some descriptive statistics\n",
    "# Your code \n",
    "#\n",
    "import pandas as pd\n",
    "pd.DataFrame(X).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xc5ljqQqxSdj"
   },
   "source": [
    "Split the data into training and test for hold-out cross-validation\n",
    "\n",
    "Note: \n",
    "\n",
    "*   stratify = y to make sure all split of the data have got the same class distribution\n",
    "\n",
    "*   by fixing the random_state, the same training/test split can be obtained (this will be useful for reproducibility) each time you run the code. If the data set is small, one can consider repeat the hold out cross-validation several times with different split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78S1iRxFsIYS",
    "outputId": "8970f016-6d47-4a30-b440-c5e4d34a422f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(341, 30) (341,)\n",
      "(228, 30) (228,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state = 21, stratify=y)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJajLQq-wVKJ"
   },
   "source": [
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "DfFauEXmCf4O",
    "outputId": "dd1863c9-c6cd-4e84-ed3a-8d93f3ec7ec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96        85\n",
      "           1       0.96      0.99      0.98       143\n",
      "\n",
      "    accuracy                           0.97       228\n",
      "   macro avg       0.97      0.96      0.97       228\n",
      "weighted avg       0.97      0.97      0.97       228\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a random forest classifier \n",
    "#Â \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=21)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_test_pred = rfc.predict(X_test)\n",
    "\n",
    "# Check the confusion matrix\n",
    "disp = plot_confusion_matrix(rfc, X_test, y_test,\n",
    "                                 display_labels=None,\n",
    "                                 cmap=plt.cm.Blues)\n",
    "\n",
    "# Just get a classification report.\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10gYmTpixVS4"
   },
   "source": [
    "Exercise: \n",
    "\n",
    "Check the training performance for the model. \n",
    "How does your model perform, is it overfitted? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "82XaccraRFYp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92105263 0.94736842 0.98245614 0.96491228 0.98230088]\n"
     ]
    }
   ],
   "source": [
    "# Your code here get check training performance\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "rfc_cross = RandomForestClassifier(n_estimators=100, random_state=21)\n",
    "crval_scores = cross_validate(rfc_cross, X, y)\n",
    "print(crval_scores['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaKTq9Kzyc8J"
   },
   "source": [
    "## Performance measures and ROC analysis\n",
    "\n",
    "Read the slides on blackboard on performance measures and ROC analysis. \n",
    "\n",
    "or check the scikit-learn doc https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics\n",
    "\n",
    "or See https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "for more detailed description on performance measures for binary classification. \n",
    "\n",
    "But before we do this, we must first venture into the wonderfully simplistic, yet incredibly powerful **Confusion Matrix**, as we already see in the practical. In case of binary classification, it is comprised of a single table with 4 combinations of predicted and actual values. \n",
    "\n",
    "- **True Positive/TP:** You have predicted positive, and that is indeed the case.\n",
    "- **True Negative/TN:** You have predicted negative, amd that is indeed the case.\n",
    "- **False Positive/FP/Type 1 Error:** You have predicted positive, where the case is actually negative.\n",
    "- **False Negative/FN/Type 2 Error:** You have predicted negative, where the case is actually positive.\n",
    "\n",
    "Using this simple metric, we are able to calculate an array of performance metrics. For example:\n",
    "\n",
    "$$Recall = {TP \\over{TP + FN}}$$\n",
    "\n",
    "Which calculates how many classes were correctly predicted for the positive task.\n",
    "\n",
    "$$Precision = {TP \\over{TP + FP}}$$\n",
    "\n",
    "Which calculates how many classes were correctly predicted for the negative task.\n",
    "\n",
    "$$F-measure = {2*(Recall*Precision) \\over{Recall + Precision}}$$\n",
    "\n",
    "Uses the harmonic mean to punish the extreme values more, where two models with low precision and high recall (or vice versa).\n",
    "\n",
    "The major calculations are the **true positive rate** or **sensitivity**:\n",
    "\n",
    "$$TPR = {TP \\over{TP+FN}}$$\n",
    "\n",
    "The **false positive rate**:\n",
    "\n",
    "$$FPR = {FP \\over{FP+TN}}$$\n",
    "\n",
    "And the **specificity**:\n",
    "\n",
    "$$Specificity = {TN \\over{TN+FP}}$$\n",
    "\n",
    "\n",
    "The **Reciever Operating Characteristic** (ROC) curve and the **Area Under the Cuve** (AUC) are very popular for evaluating the performance of binary classification models. \n",
    "\n",
    "ROC curve is constructed by plotting false positive rate vs true positive rate (specificity) at various threshold settings (for a continuous output, e.g. the probabilty output given by a decision tree model). AUC is the area under the curve. The best possible model has an AUC of 1, which means that it has a good mesaure of seperability. 0 means that the model has a worst measure of seperability, and where AUC is 0.5 it illustrates that the model has zero class seperation capacity whatsoever.\n",
    "\n",
    "You can see an example on [how to plot ROC curves in scikit-learn]( \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html#sklearn.metrics.plot_roc_curve)\n",
    "or [ROC curves with cross-validation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py)\n",
    "\n",
    "Alternatively for lower version of scikit-learn, the example plot ROC in python can be found below:\n",
    "\n",
    "**Note:** This only works for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpTQz3FoOgAe"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_auc_and_roc(y_true, scores, col='red'):\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    calc_auc = auc(fpr, tpr)\n",
    "        \n",
    "    plt.title(\"Recieving Operating Characteristic\")\n",
    "    plt.plot(fpr, tpr, col, label=\"AUC %0.3f\" % calc_auc)\n",
    "    plt.plot([0, 1], [0, 1], \"b--\")\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "tIdGMckdCbgz",
    "outputId": "8eb1bc75-2cf8-4859-a0d2-2331d4acfa88"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, plot_roc_curve\n",
    "#Â only return the probability prediction for class 1\n",
    "y_test_prob = rfc.predict_proba(X_test)[:,1]\n",
    "plot_auc_and_roc(y_test, y_test_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoJ_9fsaCf4T"
   },
   "source": [
    "## Exercise: \n",
    "\n",
    "DecisionTreeClassifier has got an attribute \"estimators_\", which is the list of individual decision trees in the forests. Randomly pick two trees to visualise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "y_GBm6zuCf4T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Using cached graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import SVG\n",
    "\n",
    "def visualise_tree(treeclf):\n",
    "    dot = export_graphviz(treeclf, filled=True, rounded=True)\n",
    "    graph = graphviz.Source(dot)\n",
    "    display(SVG(graph.pipe(format=\"svg\")))s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYGJGn3LCf4U"
   },
   "outputs": [],
   "source": [
    "# Your code here \n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6JxrMvUCf4U"
   },
   "source": [
    "## Hyperparameter tuninng using GridSearch\n",
    "\n",
    "According to Wikipedia;\n",
    "\n",
    "\"The traditional way of performing hyperparameter optimisation has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.\"\n",
    "\n",
    "When tasked with building classifiers, it is not always possible to know what the best parameters are. It is not feasible to \"brue-force\" parameters manually, so we make use of grid search. Grid Search provides us the abilit to specify a range of values from which classifiers can be built and evaluated using to find out the best combintation of parameters.\n",
    "\n",
    "Below is some example code for grid search. Define your own parameter grid and choose the 'best' hyperparameters, and refit the RF classifier to your training data, and report the test performance. \n",
    "\n",
    "GridSearch doc: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cP1LrPgxZDgy",
    "outputId": "364f4598-2a64-407a-9dc2-bea3e80a908a"
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
    "                  .format(results['mean_test_score'][candidate],\n",
    "                          results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "            \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# use a full grid over all parameters\n",
    "param_grid = {'n_estimators': [1], \"max_depth\" : [2, 8]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(rfc, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "report(grid_search.cv_results_)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5os-09YgtDwj"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Question: what is the performance metric used by default in GridSearchCV? \n",
    "#      Hint: check the parameter scoreing in documentation\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SuAt2sOZFak"
   },
   "outputs": [],
   "source": [
    "# Repeat the grid search above but choose a different metric e.g. AUC to score\n",
    "# Your code here\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00-_A9hmCf4V"
   },
   "source": [
    "If you have time, \n",
    "\n",
    "1.  Apply random forests on Titanc data\n",
    "2.  Check the feature_importance_ in the fitted RF classifier. RF's impurity based variable importantance can be used to rank the features, however with some caveat. Please see \n",
    "[this](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py)\n",
    "for experiments illustrating the issues of RF feature importance score for variables with high number of values.\n",
    "and [this](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py), for general feature selection issues when many variables are highly correlated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU9ny52h2D_Q"
   },
   "source": [
    "# Part II. Inclass Kaggle challenge\n",
    "\n",
    "The challenage will be released today but you have three weeks complete this task and to submit a prediction for automated sign-off. \n",
    "\n",
    "Register for Kaggle using your Aberystwyth University, using your AU email address. But do change your one person Team name in Kaggle with a nick name for fun and for your own privacy.\n",
    "\n",
    "https://www.kaggle.com\n",
    "\n",
    "Log in and follow the link to join the inclass competition:\n",
    "\n",
    "https://www.kaggle.com/t/5af11d013f254d3fbb32c1eca0d86ecc\n",
    "\n",
    "A major part of these practicals are to help you go out and do this for yourself. That being said, we do expect you to achieve the following:\n",
    "\n",
    "1. Load in the data, explore the data. Transform data data into a format that a machine learning technique can use.\n",
    "\n",
    "2. Might need to further split the training set into train/validation sets (depending on the sample size of the data with labels and problem at hand. Due to time limit in the class, you may skip this step for today's exercise).\n",
    "\n",
    "3. On the training set, use model selection and some automated hyperparameter tuning techniques (such as grid search or randomized search, see https://scikit-learn.org/stable/modules/grid_search.html) to select a 'best' model (i.e. the model type and the relevant hyperparameters with best performance). Initially, you might start with select one simple model and use the default settings or manual selected hyperparameters, in order to have some rough idea on problem at hand.\n",
    "\n",
    "4. Evaluate the model using an appropriate form of cross-validation, in this case providing the AUC metrics and ROC curves with cross-validation or on the validation set (if made available in the Step2). Is the performance good enough? If not go back to step 3, or repeat step 4 with a different model type and/or hyperparemters.\n",
    "\n",
    "5. Refit the 'best' model (model type and the relevant hyperparameters) to all the training data, then make prediction with probability output on the test set, save the results to a file in an appropriate format.\n",
    "\n",
    "6. Submit the prediction file to Kaggle inclass competition.\n",
    "\n",
    "7. Repeat the procedure from step 3, and see if you can improve the external public validation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDnjkwj7Cf4V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
